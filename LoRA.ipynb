{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCIfanl9ny5Z"
      },
      "outputs": [],
      "source": [
        "# Why LoRA  ?\n",
        "# well we dont want to fine tune a whole model just for a simple specific task\n",
        "# we want an adapter that attaches to the model to add specificity\n",
        "\n",
        "\n",
        "# instead of backpropagating through the whole model , for each / some weight\n",
        "#  matrices (D , K)  , we attach small two matrices (D , r) , (r , K)\n",
        "# such that their multiplication gives rise to a matrix (D , K)\n",
        "#  r << min(D,k)\n",
        "\n",
        "# notice that sizeof(mat(1000,1000)) >> sizeof(mat(1000,10)) + sizeof(mat(10,1000))\n",
        "# 1,000,000 >> 10,000 + 10,000\n",
        "# by matmul these two matrices we get a matrix with the same size as the original\n",
        "# but it is less rich in information (high in redundant information ==> lower entropy)\n",
        "\n",
        "# we want those two small matrices to hold enough information about the specificity of\n",
        "#  the task while utilizing the general information from the model weights\n",
        "# we are adding `specificity` params\n",
        "\n",
        "# thus we can use the same big / `general` weights for many adapters (different specific\n",
        "# tasks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMxUplr8JjAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained models usually have low intrinsic rank , meaning we\n",
        "# can represent the same information in a smaller space\n",
        "# for example using the singular value  decomposition\n",
        "# we get that w = u @ s @ v_t ,\n",
        "# let us define the two matrices :\n",
        "# A = v , B = u @ s\n",
        "# and we notice that the w tranformation on the vector x\n",
        "# is identical to the (B @ A) transformation on the vector x\n",
        ""
      ],
      "metadata": {
        "id": "mM7pw3Qa99kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# reproducable results (determinism)\n",
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88bGChVpKm4R",
        "outputId": "65ac460a-cd9f-4e0e-e3d2-60f1ca816b4f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f75117e19d0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307  ),(0.3081  ))\n",
        "])\n",
        "\n",
        "mnist_training = datasets.MNIST(root='./data',train=True,\n",
        "                                download=True,transform=transform)\n",
        "train_loader = DataLoader(mnist_training,batch_size = 8,shuffle=True)\n",
        "\n",
        "\n",
        "mnist_testing = datasets.MNIST(root='./data',train=False,\n",
        "                              download=True,transform=transform)\n",
        "test_loader = DataLoader(mnist_testing,batch_size=8,shuffle=True)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class OverTheMoon(nn.Module):\n",
        "    def __init__(self,hidden_size1=1000,hidden_size2=1000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(28*28,hidden_size1)\n",
        "        self.linear2 = nn.Linear(hidden_size1,hidden_size2)\n",
        "        self.linear3 = nn.Linear(hidden_size2,10)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,img):\n",
        "        # we flatten the image\n",
        "        x = img.view(-1,28*28)\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.linear2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.linear3(x)\n",
        "        # x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "model = OverTheMoon().to(device)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G1ulUjx-K6rU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader,epochs=10,total_iterations_bound = None):\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "\n",
        "    total_iters = 0\n",
        "    for epoch in range(epochs) :\n",
        "        model.train()\n",
        "        data_iterator = tqdm(dataloader,desc=f'Epoch : {epoch+1}/{epochs}')\n",
        "\n",
        "\n",
        "        total_loss = 0\n",
        "        num_iters = 0\n",
        "\n",
        "        if total_iterations_bound is not None :\n",
        "            data_iterator.total = total_iterations_bound\n",
        "\n",
        "        for batch in data_iterator :\n",
        "            num_iters += 1\n",
        "            total_iters +=1\n",
        "            x,y = batch\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(x)\n",
        "            loss = loss_fn(output,y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss/num_iters\n",
        "\n",
        "            data_iterator.set_postfix(loss=avg_loss)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if total_iterations_bound is not None and total_iters >= total_iterations_bound :\n",
        "                return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train(model, train_loader ,epochs=3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPqBNJvuNG61",
        "outputId": "db9021de-c299-43eb-a55d-356631854428"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch : 1/3: 100%|██████████| 7500/7500 [00:36<00:00, 205.79it/s, loss=0.0748]\n",
            "Epoch : 2/3: 100%|██████████| 7500/7500 [00:36<00:00, 203.59it/s, loss=0.0613]\n",
            "Epoch : 3/3: 100%|██████████| 7500/7500 [00:35<00:00, 210.32it/s, loss=0.0614]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we stash the original pre-trained weights\n",
        "\n",
        "original_weights = {}\n",
        "\n",
        "for name , param in model.named_parameters():\n",
        "    original_weights[name] = param.clone().detach()\n",
        "\n",
        "\n",
        "print('Pre-Trained Weights Stashed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZIAsdt2PV0o",
        "outputId": "589c5cd0-ff43-4321-93d4-79c14e1702c3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-Trained Weights Stashed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    miss_classifications = [ 0 for i in range(10)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(dataloader,desc='Test'):\n",
        "            x,y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            output = model(x.view(-1,784))\n",
        "\n",
        "            for idx , i in enumerate(output):\n",
        "                # if the highest logit correspond to the correct label\n",
        "                if torch.argmax(i)==y[idx]:\n",
        "                    correct += 1\n",
        "                else :\n",
        "                    miss_classifications[y[idx]] += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "    print(f'Accuracy : {round(correct/total,3)}')\n",
        "    for i in range(10):\n",
        "        print(f'Miss-classifications of the digit: {i} : {miss_classifications[i]}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test(model,test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tpf4ru2cP7SD",
        "outputId": "083938ef-f6ae-458f-e4e8-995b9de40baf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 1250/1250 [00:02<00:00, 424.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.974\n",
            "Miss-classifications of the digit: 0 : 12\n",
            "Miss-classifications of the digit: 1 : 12\n",
            "Miss-classifications of the digit: 2 : 29\n",
            "Miss-classifications of the digit: 3 : 22\n",
            "Miss-classifications of the digit: 4 : 33\n",
            "Miss-classifications of the digit: 5 : 35\n",
            "Miss-classifications of the digit: 6 : 16\n",
            "Miss-classifications of the digit: 7 : 45\n",
            "Miss-classifications of the digit: 8 : 21\n",
            "Miss-classifications of the digit: 9 : 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "summary(model,input=(8,1,28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRnjBKfKRQMa",
        "outputId": "1ee49437-6dee-46d8-f88d-c3616e6e8aa1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OverTheMoon(\n",
            "  (linear1): Linear(in_features=784, out_features=1000, bias=True)\n",
            "  (linear2): Linear(in_features=1000, out_features=1000, bias=True)\n",
            "  (linear3): Linear(in_features=1000, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "OverTheMoon                              --\n",
              "├─Linear: 1-1                            785,000\n",
              "├─Linear: 1-2                            1,001,000\n",
              "├─Linear: 1-3                            10,010\n",
              "├─ReLU: 1-4                              --\n",
              "=================================================================\n",
              "Total params: 1,796,010\n",
              "Trainable params: 1,796,010\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRAParametrization(nn.Module):\n",
        "    def __init__(self,D,K,rank=1,alpha=1,device='cpu'):\n",
        "        super().__init__()\n",
        "\n",
        "        #  in the paper they init the lora_A weights with\n",
        "        # random Gaussian\n",
        "        self.lora_A = nn.Parameter(torch.zeros((rank,K)).to(device))\n",
        "        self.lora_B = nn.Parameter(torch.zeros((D,rank)).to(device))\n",
        "\n",
        "\n",
        "        nn.init.normal_(self.lora_A,mean=0,std=1)\n",
        "\n",
        "        # HOW DID U MISS THIS LMAO\n",
        "        # simply adding the LoRA produced matrix to the initial matrix\n",
        "        # is not a controlled strategy to determine how much\n",
        "        # the LoRA contributes to the whole model ,\n",
        "        # we first normalize the with respect to the rank\n",
        "        #\n",
        "        # Because 𝐴@𝐵 can produce large values (especially if rank is small),\n",
        "        # we scale it down by rank to normalize its magnitude.\n",
        "        # then we introduce the hyperparameter alpha to control\n",
        "        # how much the LoRA contribution is amplified / down-weighted\n",
        "\n",
        "\n",
        "        self.scale = alpha / rank\n",
        "        self.enabled = True\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,original_weights):\n",
        "        # if enabled we apply on the weights only and not the biases\n",
        "        if self.enabled :\n",
        "            return original_weights  + torch.matmul(self.lora_B,self.lora_A).view(original_weights.shape)*self.scale\n",
        "\n",
        "        else  :\n",
        "            return original_weights\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MxdQojJQS28v"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW WE NEED TO EMBEDD THAT PARMETERIZATION  TO OUR MODEL\n",
        "import torch.nn.utils.parametrize as parametrize\n",
        "\n",
        "\n",
        "def linear_layer_parameterization(layer,device,rank=1,alpha=1):\n",
        "    D,K = layer.weight.shape\n",
        "\n",
        "\n",
        "\n",
        "    return LoRAParametrization(D,K,rank,alpha,device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "parametrize.register_parametrization(\n",
        "    model.linear1 , \"weight\",linear_layer_parameterization(model.linear1,device)\n",
        ")\n",
        "\n",
        "parametrize.register_parametrization(\n",
        "    model.linear2,\"weight\",linear_layer_parameterization(model.linear2,device)\n",
        ")\n",
        "\n",
        "parametrize.register_parametrization(\n",
        "    model.linear3,\"weight\",linear_layer_parameterization(model.linear3,device)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def enable_disable_lora(enabled=True):\n",
        "    for layer in [model.linear1,model.linear2,model.linear3]:\n",
        "        layer.parametrizations[\"weight\"][0].enabled = enabled\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aaE6ZekDWyRi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_parameters_lora = 0\n",
        "total_parameters_non_lora = 0\n",
        "for index, layer in enumerate([model.linear1,model.linear2,model.linear3]):\n",
        "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement()+ layer.parametrizations['weight'][0].lora_B.nelement()\n",
        "    total_parameters_non_lora += layer.weight.nelement()+layer.bias.nelement()\n",
        "\n",
        "    print(f\"\"\"\n",
        "        Layer {index+1} : W: {layer.weight.shape} + B : {layer.bias.shape}\n",
        "                          lora_A :{layer.parametrizations['weight'][0].lora_A.shape} + lora_B : {layer.parametrizations['weight'][0].lora_B.shape}\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f'Total Params with Lora : {total_parameters_lora}')\n",
        "\n",
        "print(f'Total Params non Lora : {total_parameters_non_lora}')\n",
        "\n",
        "\n",
        "Params_Ratio = ((total_parameters_lora + total_parameters_non_lora) /total_parameters_non_lora ) * 100\n",
        "\n",
        "print(f'Parameters Ratio : {1/Params_Ratio}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_PMPyxTZT8y",
        "outputId": "a7346701-46b9-4d1d-be06-c82b447caa39"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "        Layer 1 : W: torch.Size([1000, 784]) + B : torch.Size([1000])\n",
            "                          lora_A :torch.Size([1, 784]) + lora_B : torch.Size([1000, 1])\n",
            "    \n",
            "\n",
            "        Layer 2 : W: torch.Size([1000, 1000]) + B : torch.Size([1000])\n",
            "                          lora_A :torch.Size([1, 1000]) + lora_B : torch.Size([1000, 1])\n",
            "    \n",
            "\n",
            "        Layer 3 : W: torch.Size([10, 1000]) + B : torch.Size([10])\n",
            "                          lora_A :torch.Size([1, 1000]) + lora_B : torch.Size([10, 1])\n",
            "    \n",
            "Total Params with Lora : 4794\n",
            "Total Params non Lora : 1796010\n",
            "Parameters Ratio : 0.009973378557577614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW WE MUST FREEZE THE PARAMS :\n",
        "for name,param in model.named_parameters():\n",
        "    if 'lora' not in name :\n",
        "        print(f' Freezing NON-LoRA Param : {name}')\n",
        "        param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QV3xl1dboBy",
        "outputId": "2d89f235-9648-4820-8fb3-6eccadcb86d2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Freezing NON-LoRA Param : linear1.bias\n",
            " Freezing NON-LoRA Param : linear1.parametrizations.weight.original\n",
            " Freezing NON-LoRA Param : linear2.bias\n",
            " Freezing NON-LoRA Param : linear2.parametrizations.weight.original\n",
            " Freezing NON-LoRA Param : linear3.bias\n",
            " Freezing NON-LoRA Param : linear3.parametrizations.weight.original\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We notice that the model doesnt do well on numbers 7 and 9\n",
        "# NOW WE FINETUNE\n",
        "\"\"\"\n",
        "Miss-classifications of the digit: 0 : 12\n",
        "Miss-classifications of the digit: 1 : 12\n",
        "Miss-classifications of the digit: 2 : 29\n",
        "Miss-classifications of the digit: 3 : 22\n",
        "Miss-classifications of the digit: 4 : 33\n",
        "Miss-classifications of the digit: 5 : 35\n",
        "Miss-classifications of the digit: 6 : 16\n",
        "Miss-classifications of the digit: 7 : 45\n",
        "Miss-classifications of the digit: 8 : 21\n",
        "Miss-classifications of the digit: 9 : 40\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "mnist_training = datasets.MNIST(root='./data',train=True,download=True,transform=transform)\n",
        "exclude_indices = (mnist_training.targets == 7) | (mnist_training.targets == 9)\n",
        "mnist_training.data = mnist_training.data[exclude_indices]\n",
        "mnist_training.targets = mnist_training.targets[exclude_indices]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_loader = DataLoader(mnist_training,batch_size=8,shuffle=True)\n",
        "\n",
        "\n",
        "train(model,train_loader,epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1neNwkM7cwts",
        "outputId": "ccd6bb10-e886-4e25-9329-a685353b014d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch : 1/10: 100%|██████████| 1527/1527 [00:08<00:00, 180.10it/s, loss=0.0273]\n",
            "Epoch : 2/10: 100%|██████████| 1527/1527 [00:08<00:00, 189.96it/s, loss=0.00774]\n",
            "Epoch : 3/10: 100%|██████████| 1527/1527 [00:08<00:00, 186.46it/s, loss=0.00507]\n",
            "Epoch : 4/10: 100%|██████████| 1527/1527 [00:08<00:00, 176.04it/s, loss=0.00286]\n",
            "Epoch : 5/10: 100%|██████████| 1527/1527 [00:08<00:00, 188.85it/s, loss=0.00308]\n",
            "Epoch : 6/10: 100%|██████████| 1527/1527 [00:08<00:00, 186.22it/s, loss=0.00255]\n",
            "Epoch : 7/10: 100%|██████████| 1527/1527 [00:08<00:00, 182.19it/s, loss=0.00194]\n",
            "Epoch : 8/10: 100%|██████████| 1527/1527 [00:08<00:00, 190.87it/s, loss=0.00166]\n",
            "Epoch : 9/10: 100%|██████████| 1527/1527 [00:09<00:00, 158.65it/s, loss=0.0015]\n",
            "Epoch : 10/10: 100%|██████████| 1527/1527 [00:10<00:00, 142.74it/s, loss=0.0015]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5InKbg5fYha",
        "outputId": "14f58532-c887-48cc-f29f-5679cd86624f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 1250/1250 [00:03<00:00, 375.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.739\n",
            "Miss-classifications of the digit: 0 : 104\n",
            "Miss-classifications of the digit: 1 : 123\n",
            "Miss-classifications of the digit: 2 : 605\n",
            "Miss-classifications of the digit: 3 : 306\n",
            "Miss-classifications of the digit: 4 : 332\n",
            "Miss-classifications of the digit: 5 : 163\n",
            "Miss-classifications of the digit: 6 : 62\n",
            "Miss-classifications of the digit: 7 : 8\n",
            "Miss-classifications of the digit: 8 : 903\n",
            "Miss-classifications of the digit: 9 : 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THAT LORA INTEGRATION ALSO ALTERS THE OUTPUT FOR OTHER CLASSES\n",
        "\n",
        "enable_disable_lora(False)\n",
        "test(model,test_loader)\n",
        "# WE GOT THE SAME RESULTS BEFORE FINETUNNING THE LORA PARAMS\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySOnE12DftuN",
        "outputId": "e876dd35-ac6e-4a4f-eda3-65eb9c58ef5c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 1250/1250 [00:03<00:00, 395.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.974\n",
            "Miss-classifications of the digit: 0 : 12\n",
            "Miss-classifications of the digit: 1 : 12\n",
            "Miss-classifications of the digit: 2 : 29\n",
            "Miss-classifications of the digit: 3 : 22\n",
            "Miss-classifications of the digit: 4 : 33\n",
            "Miss-classifications of the digit: 5 : 35\n",
            "Miss-classifications of the digit: 6 : 16\n",
            "Miss-classifications of the digit: 7 : 45\n",
            "Miss-classifications of the digit: 8 : 21\n",
            "Miss-classifications of the digit: 9 : 40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}